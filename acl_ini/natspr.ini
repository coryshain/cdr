[data]
X_train = data/natstor_X.csv
X_dev = data/natstor_X.csv
X_test = data/natstor_X.csv
y_train = data/natstor_y_train.csv
y_dev = data/natstor_y_dev.csv
y_test = data/natstor_y_test.csv
series_ids = subject docid
split_ids = subject sentid
modulus = 4
history_length = 128
filters = fdur > 100; fdur < 3000; correct > 4; startofsentence != 1; endofsentence != 1; subjectnunique > 100

[global_settings]
outdir = results/natspr
use_gpu_if_available = False

[cdr_settings]
network_type = mle
direct_irf = False
weight_sd_init = glorot
optim_name = Nadam
epsilon = 1e-2
optim_epsilon = 1e-8
learning_rate = 1e-2
max_global_gradient_norm = 1
loss_filter_n_sds = 1000
minibatch_size = 1024
eval_minibatch_size = 10000
convergence_n_iterates = 100
n_units_input_projection = 32
n_layers_input_projection = 2
n_units_hidden_state = 32
n_units_irf = 32
n_layers_irf = 2
ema_decay = 0.999
ranef_dropout_rate = 0.2
input_projection_dropout_rate = 0.2
h_in_dropout_rate = 0.2
h_rnn_dropout_rate = 0.2
rnn_dropout_rate = 0.2
irf_dropout_rate = 0.2
nn_regularizer_name = l2_regularizer
nn_regularizer_scale = 1
ranef_regularizer_name = l2_regularizer
ranef_regularizer_scale = 10
scale_loss_with_data = True
scale_regularizer_with_data = False
rescale_time_X = True
rescale_t_delta = True
rescale_inputs = True
center_inputs = False
activation = gelu

plot_step_default = sd
plot_n_time_units = 2.5
indicator_names = endofsentence
log_freq = 1
save_freq = 10

[irf_name_map]
rate = Rate
wlen = Word length
unigramsurp = Unigram surprisal
fwprob5surp = 5-gram surprisal
fdur = RT (ms)
log(fdur) = RT (log-ms)

[model_CDRNN_FF_raw]
formula = fdur ~ wlen + unigramsurp + fwprob5surp + (1 | subject)

[model_CDRNN_RNN_raw]
n_units_rnn = inherit
formula = fdur ~ wlen + unigramsurp + fwprob5surp + (1 | subject)

[model_CDRNN_FF_log]
formula = log(fdur) ~ wlen + unigramsurp + fwprob5surp + (1 | subject)

[model_CDRNN_RNN_log]
n_units_rnn = inherit
formula = log(fdur) ~ wlen + unigramsurp + fwprob5surp + (1 | subject)

